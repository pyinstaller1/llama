
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch
import streamlit as st
import os
from datetime import datetime
from langchain_llama3 import *

st.set_page_config(page_title="ë¼ë§ˆ3", page_icon="ğŸ¦™")


# pip install streamlit kiwipiepy -U langchain-community transformers torch
# !pip install pyngrok streamlit kiwipiepy -U langchain-community
# !ngrok authtoken 2pfBXJdvp34yPDeEAVXCrvFS2aD_5guh96guiVvtwcnmEsPo9

# from pyngrok import ngrok
# public_url = ngrok.connect(8501)  # í¬íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ì§€ì •
# print(f"Public URL: {public_url}")


def get_time():
    now = datetime.now()
    return f"     [{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"

def get_time_web():
    now = datetime.now()
    return f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"


from kiwipiepy import Kiwi

kiwi = Kiwi()
stopwords = ['í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°€', 'ì´', 'ë¥¼', 'ì€', 'ëŠ”', 'ë‹¤', 'ìš”', 'ì£ ']

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ì§ˆë¬¸ì— ëŒ€í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(question):
    keywords = [
        token.form
        for token in kiwi.tokenize(question)
        if token.tag in ['NNG', 'NNP', 'VV', 'NP', 'MAG']   # ëª…ì‚¬, ë™ì‚¬, ëŒ€ëª…ì‚¬, ë¶€ì‚¬
    ]
    filtered_keywords = [word for word in keywords if word not in stopwords]  # ë¶ˆìš©ì–´ ì œê±°
    return filtered_keywords



def calculate_similarity_with_keywords8(question, vector_db, text_chunks, question_vector):
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(question)

    # ëª¨ë“  ë²¡í„° ê°€ì ¸ì˜¤ê¸°
    stored_vectors = np.array([
        vector_db.index.reconstruct(i) for i in range(vector_db.index.ntotal)
    ])
    
    # ë””ë²„ê¹…: ë²¡í„° ëª¨ì–‘ í™•ì¸
    print(f"ì§ˆë¬¸ ë²¡í„° í¬ê¸°: {np.array(question_vector).shape}")
    print(f"ì €ì¥ëœ ë²¡í„° í¬ê¸°: {stored_vectors.shape}")

    # ìœ ì‚¬ë„ ê³„ì‚°
    all_similarities_with_bonus = []
    for i, vector in enumerate(stored_vectors):
        # ë””ë²„ê¹…: í˜„ì¬ ê³„ì‚° ì¤‘ì¸ ë²¡í„° í™•ì¸
        print(f"ë²¡í„° {i + 1} í¬ê¸°: {vector.shape}")
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(
            np.array(question_vector).reshape(1, -1),
            vector.reshape(1, -1)
        )[0][0]
        
        # í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ê³„ì‚°
        keyword_bonus = sum([0.5 for keyword in keywords if keyword in text_chunks[i]])
        
        # ì´ ìœ ì‚¬ë„ ê³„ì‚°
        all_similarities_with_bonus.append(
            (i + 1, similarity + keyword_bonus)
        )

    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_similarities = sorted(all_similarities_with_bonus, key=lambda x: x[1], reverse=True)
    return sorted_similarities





# @st.cache_resource  # Streamlitì˜ cacheë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ
def load_model():
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"
    
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œì‘ " + get_time())
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ " + get_time())
    
    # GPU ê°•ì œ ìš”êµ¬
    if not torch.cuda.is_available():
        raise RuntimeError("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")

    device = torch.device("cuda")  # GPU ê°•ì œ ì‚¬ìš©

    print("ëª¨ë¸ ë¡œë“œ ì‹œì‘ " + get_time())
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        subfolder=subfolder,
        torch_dtype=torch.float16  # FP16ìœ¼ë¡œ VRAM ì ˆì•½
    ).to(device)
    
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ " + get_time())
    return tokenizer, model






if 'tokenizer' not in st.session_state:
    st.session_state['tokenizer'], st.session_state['model'] = load_model()
    print("í† í¬ë‚˜ì´ì €, ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"+get_time())    




st.title(":red[NHIS]&nbsp;ë¼ë§ˆ3 ğŸ¦™ _ì±—ë´‡_")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
  for chat_message in st.session_state["messages"]:
    st.chat_message(chat_message[0]).write(chat_message[1])

if "pipeline" not in st.session_state:
    print("ë¼ë§ˆ3 ëª¨ë¸ ë¡œë“œ ì‹œì‘   " + get_time())
    st.session_state["pipeline"] = transformers.pipeline(
        "text-generation",
        model=st.session_state["model"],
        tokenizer = st.session_state["tokenizer"],
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
    )
    st.session_state["pipeline"].model.eval()
    print("ë¼ë§ˆ3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ   " + get_time())

print("ì‹œì‘"+get_time())







with st.sidebar:
    uploaded_files = st.file_uploader("PDF ìë£Œë¥¼ ì—¬ê¸°ì— ì²¨ë¶€í•˜ì„¸ìš”.", type=['pdf'], accept_multiple_files=True)
    process = st.button("ì²¨ë¶€ëœ íŒŒì¼ ë“±ë¡")
    if process:
        if len(uploaded_files) == 0:
            st.error("íŒŒì¼ì„ ì²¨ë¶€í•˜ì„¸ìš”.")
        else:
            message_info = st.info("íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

            pdf_text = get_pdf(uploaded_files)
            text_chunks = get_text_chunks(pdf_text, st.session_state['tokenizer'])
            vector_db = get_vector_db(text_chunks)   # ë²¡í„° DBì— ì €ì¥

            for doc_id, doc in vector_db.docstore._dict.items():
              chunk_id = doc.metadata.get("chunk_id", "N/A")
              content = doc.page_content[:38].replace('\n', ' ').replace('\r', ' ')
              print(f"ì²­í¬ {int(chunk_id) + 1}: {content} ...")


            if "chain" not in st.session_state:
                st.session_state["chain"] = None

            if "retriever" not in st.session_state:
                st.session_state["retriever"] = None
            if "vector_db" not in st.session_state:
                st.session_state["vector_db"] = None
            st.session_state["vector_db"] = vector_db
            if "text_chunks" not in st.session_state:
                st.session_state["text_chunks"] = None
            st.session_state["text_chunks"] = text_chunks



            st.session_state["chain"], st.session_state["retriever"], hf_pipeline = get_chain(vector_db)

            message_info.empty()   # ì €ì¥ì´ ì™„ë£Œë˜ë©´ ì•ˆë‚´ ë©”ì„¸ì§€ ì‚­ì œ

            


            









question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
if question:
    if  len(uploaded_files) != 0:                # ì²¨ë¶€íŒŒì¼ì´ ìˆìœ¼ë©´, RAG
        st.chat_message("user").write(question + get_time_web())
        question_time = question + get_time_web()

        with st.spinner("RAG ë‹µë³€ ìƒì„± ì¤‘..."):
            from langchain.embeddings import HuggingFaceEmbeddings
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            embedding_model = HuggingFaceEmbeddings(
              model_name="jhgan/ko-sroberta-multitask",
              model_kwargs={'device': 'cpu'},
              encode_kwargs={'normalize_embeddings': True}
            )
            
            # retrieverì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ê´€ë ¨ëœ ìœ ì‚¬ë„ ê³„ì‚°
            docs = st.session_state["retriever"].get_relevant_documents(question)  # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë“¤
            
            if docs:
              question_vector = embedding_model.embed_query(question)   # ì§ˆë¬¸ ë²¡í„°
              stored_vectors = st.session_state["vector_db"].index.reconstruct_n(0, st.session_state["vector_db"].index.ntotal)   # ë²¡í„°DBì˜ PDF íŒŒì¼ ë²¡í„°
              
              # ìœ ì‚¬ë„ ê³„ì‚° (Cosine Similarity)
              similarities = [
                (doc.metadata['chunk_id'], cosine_similarity(np.array(question_vector).reshape(1, -1), np.array(stored_vectors[doc.metadata['chunk_id']]).reshape(1, -1))[0][0])
                for doc in docs
                ]
              
              # sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)
              sorted_similarities = calculate_similarity_with_keywords(question, st.session_state["vector_db"], st.session_state["text_chunks"], question_vector)
              
              for rank, (chunk_id, similarity) in enumerate(sorted_similarities, start=1):
                print(f"{rank}ìœ„ ì²­í¬ {chunk_id}: ìœ ì‚¬ë„ ì ìˆ˜ {similarity:.4f}")
            else:
              print("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")



            # answer ="RAG ì±—ë´‡ì€ 5.py ì—ì„œ ë§Œë“­ë‹ˆë‹¤."
            print(777)
            chain_result = st.session_state["chain"].run({"query": question})
            print(888)
            answer = chain_result.split("Answer:", 1)[1].strip()
            print(f"LangChain ë‹µë³€: {answer} " + get_time())
            st.chat_message("assistant").write(answer + get_time_web()) 


    if  len(uploaded_files) == 0:            # ì²¨ë¶€ íŒŒì¼ì´ ì—†ìœ¼ë©´, ì±—GPT 3.5
        st.chat_message("user").write(question + get_time_web())
        question_time = question + get_time_web()

        with st.spinner("ë¼ë§ˆ3 ëª¨ë¸ì´ ë‹µë³€ ìƒì„± ì¤‘..."):
            # answer ="ë¼ë§ˆ3 ëª¨ë¸ì´ ë‹µë³€í•©ë‹ˆë‹¤."
            answer = ask(question)
            st.chat_message("assistant").write(answer + get_time_web())

















    st.session_state["messages"].append(["user", question_time])
    st.session_state["messages"].append(["assistant", answer+get_time_web()])
    print("ë‹µë³€ ì™„ë£Œ"+get_time())




