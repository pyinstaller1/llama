



import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
import streamlit as st
from datetime import datetime
from kiwipiepy import Kiwi
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.vectorstores import FAISS

from langchain_llama3 import *



st.set_page_config(page_title="ë¼ë§ˆ3", page_icon="ğŸ¦™")


# pip install streamlit kiwipiepy -U langchain-community transformers torch scikit-learn pypdf faiss-cpu
# !pip install pyngrok streamlit pypdf faiss-cpu kiwipiepy -U langchain-community
# !ngrok authtoken 2pfBXJdvp34yPDeEAVXCrvFS2aD_5guh96guiVvtwcnmEsPo9

# from pyngrok import ngrok
# public_url = ngrok.connect(8501)  # í¬íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ì§€ì •
# print(f"Public URL: {public_url}")

# combo_chunk = "ì§ˆë¬¸:"
combo_chunk = "380"





def get_time():
    now = datetime.now()
    return f"     [{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"

def get_time_web():
    now = datetime.now()
    return f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"





kiwi = Kiwi()
stopwords = ['í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°€', 'ì´', 'ë¥¼', 'ì€', 'ëŠ”', 'ë‹¤', 'ìš”', 'ì£ ']

def extract_keywords(question):
    keywords = [
        token.form for token in kiwi.tokenize(question)
        if token.tag in ['NNG', 'NNP', 'VV', 'SL']  # ëª…ì‚¬, ê³ ìœ ëª…ì‚¬, ì™¸ë˜ì–´, ë™ì‚¬
    ]
    filtered_keywords = [word for word in keywords if word not in stopwords]
    return filtered_keywords

def calculate_similarity_with_keywords(question, vector_db, text_chunks):
    # ğŸ”¥ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ í•„í„°ë§
    stopwords = ['í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°€', 'ì´', 'ë¥¼', 'ì€', 'ëŠ”', 'ë‹¤', 'ìš”', 'ì£ ', 'í•˜']  # ğŸ”¥ 'í•˜' ì¶”ê°€
    keywords = extract_keywords(question)
    keywords = [kw for kw in keywords if kw not in stopwords and len(kw) > 1]  # ğŸ”¥ 1ê¸€ì í‚¤ì›Œë“œ ì œê±°
    print(f"ì§ˆë¬¸ í‚¤ì›Œë“œ: {keywords}")
    
    stored_vectors = np.array([vector_db.index.reconstruct(i) for i in range(vector_db.index.ntotal)])

    embedding_model = HuggingFaceEmbeddings(
      model_name="jhgan/ko-sroberta-multitask",
      model_kwargs={'device': 'cpu'},
      encode_kwargs={'normalize_embeddings': True}
    )

    question_vector = embedding_model.embed_query(question)
    
    all_similarities_with_bonus = []
    for i, vector in enumerate(stored_vectors):
        similarity = cosine_similarity(np.array(question_vector).reshape(1, -1), vector.reshape(1, -1))[0][0]
        
        # ğŸ”¥ ì²­í¬ ë‚´ ê³µë°± ì œê±° ë° í‚¤ì›Œë“œ ë§¤ì¹­
        clean_chunk = text_chunks[i].page_content.replace('\n', '').replace('\r', '').replace(' ', '')
        
        # ğŸ”¥ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ê°•í™”
        keyword_bonus = sum([(clean_chunk.count(keyword) + 2) * 1000 for keyword in keywords if keyword in clean_chunk])  # ğŸ”¥ ë³´ë„ˆìŠ¤ ê°•í™”
        weighted_similarity = similarity + keyword_bonus
        all_similarities_with_bonus.append((i, weighted_similarity))
        
        # ğŸ”¥ ì²­í¬ì˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë³´ ì¶œë ¥
        print(f"ì²­í¬ {i+1}: ìœ ì‚¬ë„ ì ìˆ˜={similarity:.4f}, í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤={keyword_bonus}, ì´ ì ìˆ˜={weighted_similarity:.4f}")
    
    # ğŸ”¥ ìœ ì‚¬ë„ì— ë”°ë¼ ì •ë ¬
    sorted_similarities = sorted(all_similarities_with_bonus, key=lambda x: x[1], reverse=True)
    return sorted_similarities






def clean_answer(answer):
    # ğŸ”¥ "Answer:" ì´í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    answer = answer.split("Answer:", 1)[-1].strip() if "Answer:" in answer else answer


    # ğŸ”¥ 'Note:', 'Context:', 'Key word:' ë’¤ì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    patterns_to_remove = ['Note:', 'Context:', 'Key word:', 'Translation:', 'Explanation:']
    for pattern in patterns_to_remove:
        if pattern in answer:
            answer = answer.split(pattern, 1)[0].strip()


    if combo_chunk == "380":
      # ğŸ”¥ ì¤‘ë³µëœ ë¬¸ì¥ ì œê±°
      sentences = answer.split('.')  # '.' ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì„ ë‚˜ëˆ”
      unique_sentences = []
      
      for sentence in sentences:
        sentence = sentence.strip()  # ê³µë°± ì œê±°
        if sentence and sentence not in unique_sentences:
          unique_sentences.append(sentence)
      answer = '. '.join(unique_sentences)  # ì¤‘ë³µ ì œê±°ëœ ë¬¸ì¥ í•©ì¹˜ê¸°

    return answer























def get_chain(vector_db):
    print("get_chain í•¨ìˆ˜ ì‹œì‘" + get_time())

    hf_pipeline = pipeline(
        "text-generation",
        model=st.session_state["model"],
        tokenizer=st.session_state["tokenizer"],
        device=0,
        framework="pt",
        return_text=True
    )
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    # combo_chunk = "ì§ˆë¬¸:"   ### 380 pdf ì—ì„œ 1ìœ„ ì ìˆ˜ì°¨ê°€ 2ë°° ì´ìƒì´ë©´ 1ê°œ, ì•„ë‹ˆë©´ 2ê°œ
    if combo_chunk == "ì§ˆë¬¸:":
      retriever = vector_db.as_retriever(search_kwargs={"k": 1})
    if combo_chunk == "380":
      retriever = vector_db.as_retriever(search_kwargs={"k": 2})

    prompt_template = PromptTemplate(
        template="Answer should be Korean. Avoid repeat of similar sentences. Use the following context to answer the question.:\n\nContext: {context}\n\n Answer:"
        )

    combine_documents_chain = load_qa_chain(llm, chain_type="stuff", prompt=prompt_template)
    chain = RetrievalQA(retriever=retriever, combine_documents_chain=combine_documents_chain)

    print("get_chain í•¨ìˆ˜ ì™„ë£Œ" + get_time())
    return chain


# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„ë¦¬
def get_text_chunks_qa(text, chunk_size=380, overlap=100):
    print("qa")


    if isinstance(text, list):
      text = " ".join([doc.page_content for doc in text])
    print(text)
    chunks = []
    start = 0
    while start < len(text):
        if "ì§ˆë¬¸:" in text[start:]:
            question_start = text.find("ì§ˆë¬¸:", start)
            answer_end = text.find("ì§ˆë¬¸:", question_start + 1)  # ë‹¤ìŒ 'ì§ˆë¬¸:' ìœ„ì¹˜ ì°¾ê¸°

            # ë‹µë³€ì˜ ëì´ ìˆë‹¤ë©´, 'ì§ˆë¬¸: ~ ë‹µë³€:' êµ¬ê°„ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì €ì¥
            if question_start != -1 and answer_end != -1:
                chunk = text[question_start:answer_end].strip()
                chunks.append(chunk)
                start = answer_end  # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡
            else:
                # ë§ˆì§€ë§‰ ì§ˆë¬¸/ë‹µë³€ì´ ëê¹Œì§€ í¬í•¨ë˜ë„ë¡ ì²˜ë¦¬
                chunk = text[question_start:].strip()
                chunks.append(chunk)
                break
        else:
            break
    
    print(7777777)
    print(chunks)
    
    chunk_documents = [
      Document(page_content=chunk, metadata={"chunk_id": idx})
      for idx, chunk in enumerate(chunks)
      ]
    
    chunks = chunk_documents
    return chunks



def get_text_chunks(text, chunk_size=380, overlap=100):
    print("no qa")

    # textê°€ Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¼ë©´, ê° Documentì˜ page_contentë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
    if isinstance(text, list):
        text = " ".join([doc.page_content for doc in text])
    
    chunks = []
    start = 0

    print(8888888)
    print(text)

    while start < len(text):
        # í˜„ì¬ ì²­í¬ì˜ ë ì¸ë±ìŠ¤ ê³„ì‚°
        end = start + chunk_size
        
        # ì²­í¬ ì €ì¥ (í˜„ì¬ ë²”ìœ„ì˜ í…ìŠ¤íŠ¸)
        chunk = text[start:end].strip()
        chunks.append(chunk)

        # ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ìœ„ì¹˜ë¡œ ì´ë™ (ì˜¤ë²„ë© ì ìš©)
        start = end - overlap

    # LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    chunk_documents = [
        Document(page_content=chunk, metadata={"chunk_id": idx})
        for idx, chunk in enumerate(chunks)
    ]

    return chunk_documents











def get_new_chain(top_chunk_ids, text_chunks):
    new_vector_db = get_new_vectcor_db(top_chunk_ids, st.session_state["vector_db"], st.session_state["text_chunks"])    
    st.session_state["chain"] = get_chain(new_vector_db)  # ğŸ”¥ ìƒˆë¡œìš´ chain ìƒì„±
    print(f"ğŸ”„ ìƒˆë¡œìš´ chainì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return st.session_state["chain"]

def get_new_vectcor_db(top_chunk_ids, vector_db, text_chunks):

    selected_documents = [text_chunks[i] for i in top_chunk_ids]  # ğŸ”¥ ìƒìœ„ ì²­í¬ë§Œ ì„ íƒ
    chunk_documents = [
        Document(page_content=doc.page_content, metadata={"chunk_id": idx}) 
        for idx, doc in enumerate(selected_documents)
    ]

    embeddings = load_embeddings()
    new_vector_db = FAISS.from_documents(chunk_documents, embeddings)  # ğŸ”¥ ìƒˆë¡œìš´ vector_db ìƒì„±
    
    print(f"ğŸ”„ ìƒˆë¡œìš´ vector_dbê°€ {len(top_chunk_ids)}ê°œì˜ ì²­í¬ë¡œ ìƒì„± ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return new_vector_db




@st.cache_resource  # Streamlitì˜ cacheë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ
def load_model():
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"
    # model_path = "C:\\model\\llama3"
    
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œì‘ " + get_time())
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ " + get_time())

    # GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    if torch.cuda.is_available():
        device = torch.device("cuda")
        torch_dtype = torch.float16  # GPUì—ì„œëŠ” FP16ìœ¼ë¡œ VRAM ì ˆì•½
        print("GPUê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    else:
        device = torch.device("cpu")
        torch_dtype = torch.float32  # CPUì—ì„œëŠ” FP32 ì‚¬ìš©
        print("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    

    print("ëª¨ë¸ ë¡œë“œ ì‹œì‘ " + get_time())
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        subfolder=subfolder,
        # model_path,
        torch_dtype=torch_dtype
    ).to(device)
    
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ " + get_time())
    return tokenizer, model



if 'tokenizer' not in st.session_state:
    st.session_state['tokenizer'], st.session_state['model'] = load_model()
    print("í† í¬ë‚˜ì´ì €, ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"+get_time())    


if "pipeline" not in st.session_state:
    print("ë¼ë§ˆ3 ëª¨ë¸ ë¡œë“œ ì‹œì‘   " + get_time())
    st.session_state["pipeline"] = transformers.pipeline(
        "text-generation",
        model=st.session_state["model"],
        tokenizer = st.session_state["tokenizer"],
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map="auto",
    )
    st.session_state["pipeline"].model.eval()
    print("ë¼ë§ˆ3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ   " + get_time())





st.title(":red[NHIS]&nbsp;ë¼ë§ˆ3 ğŸ¦™ _ì±—ë´‡_")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
  for chat_message in st.session_state["messages"]:
    st.chat_message(chat_message[0]).write(chat_message[1])



print("ì‹œì‘"+get_time())







with st.sidebar:
    uploaded_files = st.file_uploader("PDF ìë£Œë¥¼ ì—¬ê¸°ì— ì²¨ë¶€í•˜ì„¸ìš”.", type=['pdf'], accept_multiple_files=True)
    process = st.button("ì²¨ë¶€ëœ íŒŒì¼ ë“±ë¡")

    combo_chunk = st.sidebar.selectbox("ì²­í¬ êµ¬ë¶„", ["380", "ì§ˆë¬¸:"], index=0)  # ê¸°ë³¸ê°’ "380"

    if process:
        if len(uploaded_files) == 0:
            st.error("íŒŒì¼ì„ ì²¨ë¶€í•˜ì„¸ìš”.")
        else:
            message_info = st.info("íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

            pdf_text = get_pdf(uploaded_files)

            # combo_chunk = "ì§ˆë¬¸:"   ###
            if combo_chunk == "ì§ˆë¬¸:":
                text_chunks = get_text_chunks_qa(pdf_text, chunk_size=380, overlap=100)
            if combo_chunk == "380":
                text_chunks = get_text_chunks(pdf_text, chunk_size=380, overlap=100)
            
            print(8888888)
            print(text_chunks)

            vector_db = get_vector_db(text_chunks)

            for doc_id, doc in vector_db.docstore._dict.items():
                chunk_id = doc.metadata.get("chunk_id", "N/A")
                content = doc.page_content[:38].replace('\n', ' ').replace('\r', ' ')
                print(f"ì²­í¬ {int(chunk_id) + 1}: {content} ...")

            if "chain" not in st.session_state:
                st.session_state["chain"] = None
            st.session_state["chain"] = get_chain(vector_db)

            if "vector_db" not in st.session_state:
                st.session_state["vector_db"] = None
            st.session_state["vector_db"] = vector_db
            if "text_chunks" not in st.session_state:
                st.session_state["text_chunks"] = None
            st.session_state["text_chunks"] = text_chunks



            message_info.empty()   # ì €ì¥ì´ ì™„ë£Œë˜ë©´ ì•ˆë‚´ ë©”ì„¸ì§€ ì‚­ì œ

            


            









question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
if question:
    if  len(uploaded_files) != 0:                # ì²¨ë¶€íŒŒì¼ì´ ìˆìœ¼ë©´, RAG
        st.chat_message("user").write(question + get_time_web())
        question_time = question + get_time_web()

        with st.spinner("RAG ë‹µë³€ ìƒì„± ì¤‘..."):
            print(7)
            sorted_similarities = calculate_similarity_with_keywords(question, st.session_state["vector_db"], st.session_state["text_chunks"])   # ì§ˆë¬¸ê³¼ ë²¡í„°DB ì²­í¬ì˜ ìœ ì‚¬ë„ ê³„ì‚°

            top_scores = [score for _, score in sorted_similarities[:2]]
            if len(top_scores) >= 2:
                if top_scores[0] >= 2 * top_scores[1]:
                    top_chunk_ids = [chunk_id+1 for chunk_id, _ in sorted_similarities[:1]]   # ìƒìœ„ 1ê°œ ì²­í¬ ì„ íƒ
                else:
                    top_chunk_ids = [chunk_id+1 for chunk_id, _ in sorted_similarities[:2]]   # ìƒìœ„ 2ê°œ ì²­í¬ ì„ íƒ
            else:
                top_chunk_ids = [chunk_id+1 for chunk_id, _ in sorted_similarities[:1]]   # ìƒìœ„ 1ê°œ ì²­í¬ ì„ íƒ

            print(top_chunk_ids)
            get_new_chain(top_chunk_ids, st.session_state["text_chunks"])
            chain_result = st.session_state["chain"].run({"query": question})
            answer = clean_answer(chain_result.strip())

            print(f"LangChain ë‹µë³€: {answer} " + get_time())
            print("ë‹µë³€ ì™„ë£Œ" + get_time())

            print(f"LangChain ë‹µë³€: {answer} " + get_time())
            st.chat_message("assistant").write(answer + get_time_web()) 


    if  len(uploaded_files) == 0:            # ì²¨ë¶€ íŒŒì¼ì´ ì—†ìœ¼ë©´, ë¼ë§ˆ3 LLMì´ ë‹µë³€
        st.chat_message("user").write(question + get_time_web())
        question_time = question + get_time_web()

        with st.spinner("ë¼ë§ˆ3 ëª¨ë¸ì´ ë‹µë³€ ìƒì„± ì¤‘..."):
            answer = ask(question)
            st.chat_message("assistant").write(answer + get_time_web())

















    st.session_state["messages"].append(["user", question_time])
    st.session_state["messages"].append(["assistant", answer+get_time_web()])
    print("ë‹µë³€ ì™„ë£Œ"+get_time())



