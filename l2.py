
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st
import os
from datetime import datetime
from langchain_llama3 import *


def get_time():
    now = datetime.now()
    return f"     [{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"

def get_time_web():
    now = datetime.now()
    return f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"

@st.cache_resource  # Streamlitì˜ cacheë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ
def load_model():
    print("ëª¨ë¸ ë¡œë“œ ì‹œì‘"+get_time())
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ"+get_time())
    model = AutoModelForCausalLM.from_pretrained(model_name, subfolder=subfolder)
    return tokenizer, model



@st.cache_resource  # Streamlitì˜ cacheë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ
def load_tokenizer():
    print("ëª¨ë¸ ë¡œë“œ ì‹œì‘"+get_time())
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ"+get_time())
    return tokenizer







st.set_page_config(page_title="ë¼ë§ˆ3", page_icon="ğŸ¦™")
st.title(":red[NHIS]&nbsp;ë¼ë§ˆ3 ğŸ¦™ _ì±—ë´‡_")


if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message[0]).write(chat_message[1])

# if 'tokenizer' not in st.session_state or 'model' not in st.session_state:
#    st.session_state['tokenizer'], st.session_state['model'] = load_model()
#    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"+get_time())

if 'tokenizer' not in st.session_state:
    st.session_state['tokenizer'] = load_tokenizer()
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ"+get_time())    


print("ì‹œì‘"+get_time())






with st.sidebar:
    uploaded_files = st.file_uploader("PDF ìë£Œë¥¼ ì—¬ê¸°ì— ì²¨ë¶€í•˜ì„¸ìš”.", type=['pdf'], accept_multiple_files=True)
    process = st.button("ì²¨ë¶€ëœ íŒŒì¼ ë“±ë¡")
    if process:
        if len(uploaded_files) == 0:
            st.error("íŒŒì¼ì„ ì²¨ë¶€í•˜ì„¸ìš”.")
        else:
            message_info = st.info("íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

            # ê° ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
            pdf_text = ""
            for file in uploaded_files:
                pdf_text += " ".join([doc.page_content for doc in get_pdf(file)])

            text_chunks = get_text_chunks(pdf_text, st.session_state['tokenizer'])

            vector_db = get_vector_db(text_chunks)   # ë²¡í„° DBì— ì €ì¥

            print("ì²­í¬ ìˆ˜:", vector_db.index.ntotal)  # ì¸ë±ìŠ¤ì— ì €ì¥ëœ ë²¡í„°ì˜ ê°œìˆ˜ ì¶œë ¥
            for i, (doc_id, document) in enumerate(vector_db.docstore._dict.items()):
                print(f"ì²­í¬ {i}: {document.page_content[:35]}".replace("\n", " ").replace("\r", " "))

            print(777)
            if "chain" not in st.session_state:
                st.session_state["chain"] = None
            st.session_state["chain"] = get_chain(vector_db)
            print(888)



            message_info.empty()   # ì €ì¥ì´ ì™„ë£Œë˜ë©´ ì•ˆë‚´ ë©”ì„¸ì§€ ì‚­ì œ

            


            









question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
if question:
    st.chat_message("user").write(question + get_time_web())



    if  len(uploaded_files) != 0:                # ì²¨ë¶€íŒŒì¼ì´ ìˆìœ¼ë©´, RAG
        st.chat_message("user").write(question)

        with st.spinner("RAG ë‹µë³€ ìƒì„± ì¤‘..."):

            # 5.pyì—ì„œ ì´ ë¶€ë¶„ì— RAG ì±—ë´‡ì„ ë§Œë“­ë‹ˆë‹¤.

            answer ="RAG ì±—ë´‡ì€ 5.py ì—ì„œ ë§Œë“­ë‹ˆë‹¤."
            st.chat_message("assistant").write(answer) 


    if  len(uploaded_files) == 0:            # ì²¨ë¶€ íŒŒì¼ì´ ì—†ìœ¼ë©´, ì±—GPT 3.5
        st.chat_message("user").write(question)
        print(777)

        with st.spinner("ë¼ë§ˆ3 ëª¨ë¸ì´ ë‹µë³€ ìƒì„± ì¤‘..."):
            print(77777777777)

            answer ="ë¼ë§ˆ3 ëª¨ë¸ì´ ë‹µë³€í•©ë‹ˆë‹¤."
            st.chat_message("assistant").write(answer)
            
            # key = os.getenv('OPENAI_KEY')
            # llm = ChatOpenAI(openai_api_key = key)   # ì±—GPTì—ê²Œ ì§ˆë¬¸ (PDFê°€ ì—†ì–´ì„œ RAG ì•„ë‹˜)
            # answer = llm.invoke(question)                # ì±—GPTì—ê²Œ ë‹µë³€ì„ ë°›ì•„ì˜´ (RAG ì•„ë‹˜)
            # answer = answer.content
            # st.chat_message("assistant").write(answer)   # ì±—GPTì˜ ë‹µë³€ ì¶œë ¥



    """
    # tokenizerì˜ __call__ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ì…ë ¥ê³¼ attention_maskë¥¼ í•¨ê»˜ ê°€ì ¸ì˜´
    inputs = st.session_state['tokenizer'](question, return_tensors="pt")
    print("ì§ˆë¬¸ ì—”ì½”ë”© ì™„ë£Œ" + get_time())

    # ëª¨ë¸ì— ì…ë ¥ê°’ê³¼ attention_maskë¥¼ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ ìƒì„±
    outputs = st.session_state['model'].generate(
        inputs["input_ids"], 
        max_length=300,
        pad_token_id=st.session_state['tokenizer'].pad_token_id,
        attention_mask=inputs["attention_mask"],
        temperature=0.7,        # ìƒì„±ì˜ ë‹¤ì–‘ì„±ì„ ì•½ê°„ ì¶”ê°€
        top_p=0.9,              # ì„ íƒ ë²”ìœ„ë¥¼ ë„“í˜€ ë³´ë‹¤ í’ë¶€í•œ ë‹µë³€ ìƒì„±
        repetition_penalty=1.2  # ë°˜ë³µì„ ì–µì œí•˜ì—¬ ë‹µë³€ì´ ë‹¤ì–‘í•´ì§
    )
    print("ë‹µë³€ ë²¡í„° ìƒì„± output ì™„ë£Œ" + get_time())

    answer = st.session_state['tokenizer'].decode(outputs[0], skip_special_tokens=True)
                                                
    st.chat_message("assistant").write(answer+get_time_web())
    """
















    st.session_state["messages"].append(["user", question+get_time_web()])
    st.session_state["messages"].append(["assistant", answer+get_time_web()])
    print("ë‹µë³€ ì™„ë£Œ"+get_time())






