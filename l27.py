
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
import os
from datetime import datetime
from langchain_llama3 import *

# !pip install pyngrok streamlit pypdf faiss-gpu kiwipiepy -U langchain-community



def get_time():
    now = datetime.now()
    return f"     [{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"

def get_time_web():
    now = datetime.now()
    return f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"








from kiwipiepy import Kiwi

kiwi = Kiwi()
stopwords = ['í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°€', 'ì´', 'ë¥¼', 'ì€', 'ëŠ”', 'ë‹¤', 'ìš”', 'ì£ ']

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np



# ğŸ”¥ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(question):
    keywords = [
        token.form for token in kiwi.tokenize(question)
        if token.tag in ['NNG', 'NNP', 'VV', 'SL']  # ëª…ì‚¬, ê³ ìœ ëª…ì‚¬, ì™¸ë˜ì–´, ë™ì‚¬
    ]
    filtered_keywords = [word for word in keywords if word not in stopwords]
    return filtered_keywords




from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def calculate_similarity_with_keywords(question, vector_db, text_chunks, question_vector):
    # ğŸ”¥ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ í•„í„°ë§
    stopwords = ['í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ê°€', 'ì´', 'ë¥¼', 'ì€', 'ëŠ”', 'ë‹¤', 'ìš”', 'ì£ ', 'í•˜']  # ğŸ”¥ 'í•˜' ì¶”ê°€
    keywords = extract_keywords(question)
    keywords = [kw for kw in keywords if kw not in stopwords and len(kw) > 1]  # ğŸ”¥ 1ê¸€ì í‚¤ì›Œë“œ ì œê±°
    print(f"ì§ˆë¬¸ í‚¤ì›Œë“œ: {keywords}")
    
    stored_vectors = np.array([vector_db.index.reconstruct(i) for i in range(vector_db.index.ntotal)])
    
    all_similarities_with_bonus = []
    for i, vector in enumerate(stored_vectors):
        similarity = cosine_similarity(np.array(question_vector).reshape(1, -1), vector.reshape(1, -1))[0][0]
        
        # ğŸ”¥ ì²­í¬ ë‚´ ê³µë°± ì œê±° ë° í‚¤ì›Œë“œ ë§¤ì¹­
        clean_chunk = text_chunks[i].page_content.replace('\n', '').replace('\r', '').replace(' ', '')
        
        # ğŸ”¥ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ê°•í™” (ë³´ë„ˆìŠ¤ì˜ í¬ê¸°ë¥¼ ë” ë†’ê²Œ ì¡°ì •)
        keyword_bonus = sum([(clean_chunk.count(keyword) + 2) * 200 for keyword in keywords if keyword in clean_chunk])  # ğŸ”¥ ë³´ë„ˆìŠ¤ ê°•í™”
        
        # ğŸ”¥ ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì¡°ì •
        weighted_similarity = 0.2 * similarity + 0.8 * keyword_bonus  # ğŸ”¥ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„ì¤‘ì„ 0.2ë¡œ ì¦ê°€
        
        all_similarities_with_bonus.append((i, weighted_similarity))
        
        # ğŸ”¥ ì²­í¬ì˜ í‚¤ì›Œë“œ ë§¤ì¹­ ì •ë³´ ì¶œë ¥
        print(f"ì²­í¬ {i+1}: ìœ ì‚¬ë„ ì ìˆ˜={similarity:.4f}, í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤={keyword_bonus}, ì´ ì ìˆ˜={weighted_similarity:.4f}")
    
    # ğŸ”¥ ìœ ì‚¬ë„ì— ë”°ë¼ ì •ë ¬
    sorted_similarities = sorted(all_similarities_with_bonus, key=lambda x: x[1], reverse=True)
    return sorted_similarities






























"""
@st.cache_resource  # Streamlitì˜ cacheë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ
def load_tokenizer():
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œì‘"+get_time())
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ"+get_time())
    return tokenizer
"""



@st.cache_resource  # Streamlitì˜ cacheë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œ
def load_model():
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œì‘"+get_time())
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ"+get_time())
    model = AutoModelForCausalLM.from_pretrained(model_name, subfolder=subfolder)
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"+get_time())
    return tokenizer, model


if 'tokenizer' not in st.session_state:
    # st.session_state['tokenizer'] = load_tokenizer()
    st.session_state['tokenizer'], st.session_state['model'] = load_model()
    print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ"+get_time())    






"""


def get_time():
    now = datetime.now()
    return f"     [{now.hour}ì‹œ{now.minute}ë¶„{now.second}ì´ˆ]"



from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime






if "tokenizer" in globals():
    del globals()["tokenizer"]
if "model" in globals():
    del globals()["model"]

# ë˜ëŠ” __builtins__ì—ì„œ ì‚­ì œ
if hasattr(__builtins__, "tokenizer"):
    del __builtins__.tokenizer
if hasattr(__builtins__, "model"):
    del __builtins__.model

print("ê¸°ì¡´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì‚­ì œ ì™„ë£Œ")

if "get_chain" in globals():
    del globals()["get_chain"]
print("ê¸°ì¡´ get_chain í•¨ìˆ˜ ì‚­ì œ ì™„ë£Œ")











# ëª¨ë¸ ë¡œë“œ ë° ì „ì—­ ë³€ìˆ˜ ì •ì˜
if "tokenizer" not in globals() or "model" not in globals():
    print("ëª¨ë¸ ë¡œë“œ ì‹œì‘" + get_time())
    model_name = "verygood7/llama3-ko-8b"
    subfolder = "snapshots/10acb1aa4f341f2d3c899d78c520b0822a909b95"

    # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder=subfolder)
    model = AutoModelForCausalLM.from_pretrained(model_name, subfolder=subfolder)

    # ìµœìƒìœ„ ì „ì—­ ë³€ìˆ˜ë¡œ ë“±ë¡
    __builtins__.tokenizer = tokenizer
    __builtins__.model = model

    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ" + get_time())
else:
    print("ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©")

"""





"""

print("ëª¨ë¸ ë¡œë“œ ì‹œì‘")
model = st.session_state["model"]
print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œì‘")
tokenizer = st.session_state["tokenizer"]
print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤." + get_time())
"""
















from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from transformers import pipeline
from langchain.llms import HuggingFacePipeline

def get_chain(vector_db):
    print("get_chain í•¨ìˆ˜ ì‹œì‘" + get_time())

    hf_pipeline = pipeline(
        "text-generation",
        model=st.session_state["model"],
        tokenizer=st.session_state["tokenizer"],
        device=0,
        framework="pt",
        return_text=True
    )
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    retriever = vector_db.as_retriever()   # as_retriever(search_kwargs={"k": 5})   ì²­í¬5ê°œ

    prompt_template = PromptTemplate(
        template="Use the following context to answer the question:\n\nContext: {context}\n\n Answer:"
        )

    combine_documents_chain = load_qa_chain(llm, chain_type="stuff", prompt=prompt_template)
    chain = RetrievalQA(retriever=retriever, combine_documents_chain=combine_documents_chain)

    print("get_chain í•¨ìˆ˜ ì™„ë£Œ" + get_time())
    return chain, retriever












from io import BytesIO

# Streamlitì˜ UploadedFileê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ëª¨ë°©í•œ í´ë˜ìŠ¤
class UploadedFile:
    def __init__(self, name, content):
        self.name = name  # íŒŒì¼ ì´ë¦„
        self.content = content  # íŒŒì¼ ë‚´ìš© (ë°”ì´ë„ˆë¦¬ ë°ì´í„°)

    def read(self):
        """Streamlitì˜ UploadedFileì²˜ëŸ¼ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ë°˜í™˜"""
        return self.content

uploaded_files = [
    UploadedFile(
        name="ê±´ê°•ë³´í—˜ìë£Œ.pdf",
        content=open("./ê±´ê°•ë³´í—˜ìë£Œ.pdf", "rb").read()
    )
]
















pdf_text = get_pdf(uploaded_files)
# text_content = " ".join(doc.page_content for doc in pdf_text)  # Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ -> ë¬¸ìì—´ ë³€í™˜



from langchain.schema import Document  # Document í´ë˜ìŠ¤ ì¶”ê°€

text_content = [Document(page_content=doc.page_content, metadata={"source": "pdf", "page_number": idx}) 
                for idx, doc in enumerate(pdf_text)]



text_chunks = get_text_chunks(text_content, st.session_state['tokenizer'])
# text_chunks = get_text_chunks(text_content, tokenizer)
vector_db = get_vector_db(text_chunks)

for doc_id, doc in vector_db.docstore._dict.items():
    chunk_id = doc.metadata.get("chunk_id", "N/A")
    content = doc.page_content[:38].replace('\n', ' ').replace('\r', ' ')
    print(f"ì²­í¬ {int(chunk_id) + 1}: {content} ...")






from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

template_input="Use the following context to answer the question:\n\nContext: {context}\n\nAnswer:"
# template_input="ë‹µë³€ì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ì§ˆë¬¸ ì† í‚¤ì›Œë“œ({keywords})ë¥¼ ê°•ì¡°í•˜ì—¬ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."


# st.session_state["model"] = model
# st.session_state["tokenizer"] = tokenizer

if "chain" not in st.session_state:
  st.session_state["chain"] = None
st.session_state["chain"], retriever = get_chain(vector_db)
chain = st.session_state["chain"]










from langchain.embeddings import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)



# ğŸ”¥ ì§ˆë¬¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
question = "ì¥ê¸°ë ¤ê°€ ë­í–ˆëƒ?"  # ì˜ˆì‹œ ì§ˆë¬¸

# ğŸ”¥ ì§ˆë¬¸ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
question_vector = embedding_model.embed_query(question)  # ğŸ”¥ ì§ˆë¬¸ ë²¡í„° ìƒì„±

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


# ğŸ”¥ retrieverì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ê´€ë ¨ëœ ìœ ì‚¬ë„ ê³„ì‚°
docs = retriever.get_relevant_documents(question)  # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë“¤

if docs:
    # ğŸ”¥ 1ï¸âƒ£ ì§ˆë¬¸ ë²¡í„° ìƒì„± (ì¤‘ë³µ ì œê±°)
    stored_vectors = vector_db.index.reconstruct_n(0, vector_db.index.ntotal)   # ë²¡í„°DBì˜ PDF íŒŒì¼ ë²¡í„°

    # ğŸ”¥ 2ï¸âƒ£ ìœ ì‚¬ë„ ê³„ì‚° (Cosine Similarity)
    similarities = [
        (doc.metadata.get('chunk_id'), cosine_similarity(
            np.array(question_vector).reshape(1, -1), 
            np.array(stored_vectors[doc.metadata.get('chunk_id', 0)]).reshape(1, -1)
        )[0][0]) 
        for doc in docs if doc.metadata.get('chunk_id') is not None
    ]

    # ğŸ”¥ 3ï¸âƒ£ ì²­í¬ì™€ ìœ ì‚¬ë„ ì •ë ¬

    question = "ì¥ê¸°ë ¤ê°€ ë­í–ˆëƒ?"
    sorted_similarities = calculate_similarity_with_keywords(question, vector_db, text_chunks, question_vector)

    for rank, (chunk_id, similarity) in enumerate(sorted_similarities, start=1):
        print(f"{rank}ìœ„ ì²­í¬ {chunk_id + 1}: ìœ ì‚¬ë„ ì ìˆ˜ {similarity:.4f}")  # ğŸ”¥ chunk_id + 1ë¡œ 1ë¶€í„° ì‹œì‘

    # ğŸ”¥ 4ï¸âƒ£ ìƒìœ„ 4ê°œ ì²­í¬ ì„ íƒ
    top_chunks = [chunk_id for chunk_id, similarity in sorted_similarities[:4] if chunk_id < len(stored_vectors)]  # ìƒìœ„ 4ê°œ ì²­í¬ ì„ íƒ
    print(f"ğŸ’ ì„ íƒëœ ì²­í¬ ID: {[chunk_id + 1 for chunk_id in top_chunks]}")  # ğŸ”¥ 1ë¶€í„° ì‹œì‘

    # ğŸ“š 5ï¸âƒ£ vector_dbì— ì €ì¥ëœ ëª¨ë“  í‚¤ í™•ì¸
    print(f"ğŸ“š vector_dbì— ì €ì¥ëœ í‚¤: {list(vector_db.docstore._dict.keys())}")

    # ğŸ“˜ 6ï¸âƒ£ chunk_idì™€ UUID í‚¤ì˜ ë§¤í•‘ í…Œì´ë¸” ìƒì„±
    chunk_id_to_uuid = {}

    for key, doc in vector_db.docstore._dict.items():
        if 'chunk_id' in doc.metadata:
            chunk_id = int(doc.metadata['chunk_id'])  # ğŸ”¥ chunk_idê°€ ë¬¸ìì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ intë¡œ ë³€í™˜
            chunk_id_to_uuid[chunk_id] = key  # chunk_id -> UUID ë§¤í•‘
        else:
            print(f"âš ï¸ 'chunk_id'ê°€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤. key: {key}, metadata: {doc.metadata}")

    print(f"ğŸ“˜ chunk_id_to_uuid ë§¤í•‘: {chunk_id_to_uuid}")

    # ğŸ”¥ 7ï¸âƒ£ retrieverì˜ ë¬¸ì„œ ëª©ë¡ì„ ìš°ë¦¬ê°€ ì„ íƒí•œ ì²­í¬ë¡œ êµì²´
    try:
        docs = []
        for chunk_id in top_chunks:
            if chunk_id in chunk_id_to_uuid:
                # ğŸ”¥ ë§¤í•‘ëœ UUIDë¡œ docstoreì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                uuid_key = chunk_id_to_uuid[chunk_id]
                docs.append(vector_db.docstore._dict[uuid_key])
            else:
                print(f"âŒ KeyError: chunk_id '{chunk_id + 1}'ì— í•´ë‹¹í•˜ëŠ” í‚¤ê°€ vector_dbì— ì—†ìŠµë‹ˆë‹¤.")
    except KeyError as e:
        print(f"âŒ KeyError: {e}. ğŸ” ë‹¤ì‹œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
        print(f"ğŸ” ê°€ëŠ¥í•œ í‚¤ ëª©ë¡: {list(vector_db.docstore._dict.keys())}")
else:
    print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")











import re

def clean_answer(answer):
    """
    Answer ë¶€ë¶„ì˜ ë¶ˆí•„ìš”í•œ [Your Website], [Your Blog] ê°™ì€ ë¶€ë¶„ì„ ì œê±°í•˜ê³ 
    ì¤‘ë³µëœ ê³µë°±ì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜.
    """
    # ğŸ”¥ Answer: ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    if "Answer:" in answer:
        answer = answer.split("Answer:", 1)[1].strip()

    # ğŸ”¥ ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ [Your Website] ê°™ì€ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
    answer = re.sub(r'\[.*?\]', '', answer)  # ğŸ”¥ ëŒ€ê´„í˜¸ [ ] ì•ˆì˜ ëª¨ë“  ë‚´ìš© ì œê±°

    # ğŸ”¥ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆê³¼ ê³µë°± ì •ë¦¬
    answer = re.sub(r'\s+', ' ', answer)  # ğŸ”¥ ì—¬ëŸ¬ ê°œì˜ ê³µë°±, ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ í†µí•©
    return answer.strip()


def update_chain_with_top_chunks(top_chunks, text_chunks):
    """retrieverì™€ chainì„ ì—…ë°ì´íŠ¸í•˜ê³  ìƒˆë¡œìš´ chainìœ¼ë¡œ êµì²´"""
    # ğŸ”¥ update_retriever_with_top_chunks í•¨ìˆ˜ë¡œ retriever ìƒì„±
    retriever = update_retriever_with_top_chunks(top_chunks, vector_db, text_chunks)
    
    # ğŸ”¥ ìƒˆë¡œìš´ chain ìƒì„±
    chain, _ = get_chain(retriever)  # ğŸ”¥ ìƒˆë¡œìš´ chain ìƒì„± (1ë²ˆë§Œ í˜¸ì¶œ)
    st.session_state["chain"] = chain  # ğŸ”¥ ìƒˆë¡œìš´ ì²´ì¸ì„ ì„¸ì…˜ì— ë°˜ì˜
    
    print(f"ğŸ”„ ìƒˆë¡œìš´ chainì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return chain



def get_top_context_from_chunks(top_chunk_ids, text_chunks, top_score, second_score):
    """1ìœ„ ì²­í¬ì˜ ì „ì²´ ë‚´ìš©ì„ í¬í•¨í•˜ê³ , ë‚˜ë¨¸ì§€ ì²­í¬ëŠ” ìš”ì•½í•˜ì—¬ ë°˜í™˜"""
    from kiwipiepy import Kiwi
    kiwi = Kiwi()

    context_parts = []

    # ğŸ”¥ 1ìœ„ ì²­í¬ì˜ ë‚´ìš© ì¶”ê°€
    top_chunk_id = top_chunk_ids[0]  # 1ìœ„ ì²­í¬
    top_chunk_text = text_chunks[top_chunk_id].page_content
    context_parts.append(f"ê°€ì¥ ì¤‘ìš”í•œ ì°¸ê³  ë¬¸ì„œ:\n{top_chunk_text}\n")

    # ğŸ”¥ 1ìœ„ì™€ 2ìœ„ì˜ ì ìˆ˜ ì°¨ì´ê°€ í´ ê²½ìš°, 1ìœ„ë§Œ ìì„¸íˆ ì°¸ì¡°
    if top_score > second_score * 2:  # 1ìœ„ ì ìˆ˜ê°€ 2ìœ„ì˜ 2ë°° ì´ìƒì´ë©´
        print(f"âš ï¸ 1ìœ„ ì²­í¬ì˜ ì ìˆ˜ê°€ 2ìœ„ë³´ë‹¤ 2ë°° ë†’ìŒ. 1ìœ„ ì²­í¬ë§Œ ì°¸ì¡°í•©ë‹ˆë‹¤.")
        return "\n".join(context_parts)  # ğŸ”¥ 1ìœ„ ì²­í¬ë§Œ ë°˜í™˜

    # ğŸ”¥ ë‚˜ë¨¸ì§€ 2~4ìœ„ ì²­í¬ì˜ ìš”ì•½ ì¶”ê°€
    for chunk_id in top_chunk_ids[1:]:
        chunk_text = text_chunks[chunk_id].page_content
        sentences = chunk_text.split('.')

        # ğŸ”¥ í‚¤ì›Œë“œ í•„í„°ë§ìœ¼ë¡œ ì¤‘ìš”í•œ ë¬¸ì¥ë§Œ ì¶”ì¶œ
        relevant_sentences = [sentence for sentence in sentences if len(sentence) > 10]  # ê°„ë‹¨í•œ í•„í„°
        context_parts.append(f"ì°¸ê³  ë¬¸ì„œ ìš”ì•½ (ì²­í¬ {chunk_id + 1}):\n" + " ".join(relevant_sentences[:2]) + "\n")

    return "\n".join(context_parts)




def update_retriever_with_top_chunks(top_chunk_ids, vector_db, text_chunks):
    """ì„ íƒëœ ì²­í¬ë¡œ ìƒˆë¡œìš´ vector_db ë° retriever ìƒì„±"""
    from langchain.vectorstores import FAISS
    from langchain.schema import Document

    # ğŸ”¥ ì„ íƒëœ ì²­í¬ì˜ ë‚´ìš©ì„ ì¶”ì¶œ
    selected_documents = [text_chunks[i] for i in top_chunk_ids]  # ğŸ”¥ ìƒìœ„ ì²­í¬ë§Œ ì„ íƒ
    chunk_documents = [
        Document(page_content=doc.page_content, metadata={"chunk_id": idx}) 
        for idx, doc in enumerate(selected_documents)
    ]

    # ğŸ”¥ ìƒˆë¡œìš´ vector_db ìƒì„± (ìƒìœ„ ì²­í¬ë¡œë§Œ ìƒì„±)
    embeddings = load_embeddings()
    new_vector_db = FAISS.from_documents(chunk_documents, embeddings)  # ğŸ”¥ ìƒˆë¡œìš´ vector_db ìƒì„±
    
    # ğŸ”¥ retrieverë¥¼ new_vector_dbë¡œ ì§ì ‘ ì‚¬ìš©
    retriever = new_vector_db  # ğŸ”¥ new_vector_dbëŠ” ì´ë¯¸ retrieverë¡œ ì‚¬ìš© ê°€ëŠ¥

    print(f"ğŸ”„ ìƒˆë¡œìš´ retrieverê°€ {len(top_chunk_ids)}ê°œì˜ ì²­í¬ë¡œ ì—…ë°ì´íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return retriever  # ğŸ”¥ retrieverë§Œ ë°˜í™˜












# ğŸ”¥ ì²­í¬ ìœ ì‚¬ë„ ê³„ì‚°
sorted_similarities = calculate_similarity_with_keywords(question, vector_db, text_chunks, question_vector)

# ğŸ”¥ ìƒìœ„ 4ê°œ ì²­í¬ ì„ íƒ
top_chunks = [chunk_id for chunk_id, _ in sorted_similarities[:4]]
top_scores = [score for _, score in sorted_similarities[:4]]  # ğŸ”¥ ê° ì²­í¬ì˜ ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ì¶œ

# ğŸ”¥ ìƒˆë¡œìš´ retrieverì™€ chain ìƒì„±
update_chain_with_top_chunks(top_chunks, text_chunks)

# ğŸ”¥ ìƒìœ„ ì²­í¬ì˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¤‘ìš”í•œ ë¬¸ì¥ë§Œ ì¶”ì¶œ)
keywords = extract_keywords(question)
context = get_top_context_from_chunks(top_chunks, text_chunks, top_scores[0], top_scores[1])  # ğŸ”¥ ì ìˆ˜ ì¶”ê°€

# ğŸ”¥ LLMì— ì§ˆë¬¸ì„ ëª…í™•íˆ ì „ë‹¬í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
prompt = f"""
ì•„ë˜ì˜ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

{context}

ì§ˆë¬¸: {question}

ë‹µë³€: 
"""

# ğŸ”¥ chain.runìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
chain_result = st.session_state["chain"].run({"query": prompt})

# ğŸ”¥ ê²°ê³¼ê°€ ë¬¸ìì—´ì¼ ê²½ìš° ë°”ë¡œ ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë”•ì…”ë„ˆë¦¬ë¡œ ì ‘ê·¼
if isinstance(chain_result, str):
    answer = chain_result.strip()
else:
    answer = chain_result.get('result', 'ë‹µë³€ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.').strip()

# ğŸ”¥ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
cleaned_answer = clean_answer(answer)

print(f"LangChain ë‹µë³€: {cleaned_answer} " + get_time())
print("ë‹µë³€ ì™„ë£Œ" + get_time())



